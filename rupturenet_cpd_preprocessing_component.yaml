name: RuptureNet_CPD_Preprocessing
description: Preprocesses time series data for RuptureNet CPD model - handles feature engineering, outlier detection, train/test splitting, and standardization using StandardScaler.
inputs:
  - {name: input_csv, type: Dataset, description: "Path to input CSV file containing rupturenet features (1980-2024)"}
  - {name: min_years, type: Integer, default: "15", description: "Minimum years required for valid time series"}
  - {name: test_split_pct, type: Float, default: "0.15", description: "Percentage of data to use for test set"}
  - {name: outlier_log_threshold, type: Float, default: "1000.0", description: "Range threshold for log transformation"}
  - {name: outlier_clip_threshold, type: Float, default: "100.0", description: "Range threshold for outlier clipping"}
outputs:
  - {name: processed_data, type: Dataset, description: "Directory containing all processed data files"}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pandas || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pandas --user
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet numpy || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet numpy --user
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet scikit-learn || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import StandardScaler
        import json
        from datetime import datetime
        import pickle
        import warnings
        import os
        import argparse
        warnings.filterwarnings('ignore')

        # Core features for the model
        CORE_FEATURES = [
            'wb_gdp_current_usd', 'trade_total_usd', 'trade_intensity',
            'trade_balance_pct_gdp', 'export_import_ratio', 'patent_intensity',
            'patents_per_billion_gdp', 'tech_diversity_index', 'pestle_economic',
            'pestle_social', 'pestle_technology', 'pestle_overall',
            'wb_gdp_growth_annual_pct', 'wb_fdi_net_inflows_pct_gdp',
            'innovation_capacity_index', 'wb_gdp_current_usd_pct_change_1y',
            'wb_gdp_current_usd_rolling_std_5y', 'trade_intensity_zscore_5y',
            'patent_intensity_pct_change_1y', 'trade_shock_signal',
            'innovation_surge_signal', 'fdi_shock_signal', 'composite_rupture_signal',
        ]

        if __name__ == "__main__":
            parser = argparse.ArgumentParser()
            parser.add_argument('--input_csv', type=str, required=True)
            parser.add_argument('--min_years', type=int, default=15)
            parser.add_argument('--test_split_pct', type=float, default=0.15)
            parser.add_argument('--outlier_log_threshold', type=float, default=1000.0)
            parser.add_argument('--outlier_clip_threshold', type=float, default=100.0)
            parser.add_argument('--processed_data', type=str, required=True)
            args = parser.parse_args()

            try:
                print("="*80)
                print("RUPTURENET-CPD: DATA PREPROCESSING FOR MODEL TRAINING")
                print("="*80)
                print(f"\nStarted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

                # Determine input file path
                if os.path.isdir(args.input_csv):
                    input_file = os.path.join(args.input_csv, 'rupturenet_features_final_1980_2024.csv')
                else:
                    input_file = args.input_csv

                print(f"\n{'='*80}")
                print(f"LOADING DATASET")
                print(f"{'='*80}")

                df = pd.read_csv(input_file)
                print(f"\n‚úì Loaded: {df.shape}")
                print(f"  Columns: {list(df.columns)[:10]}...")

                print(f"\n{'='*80}")
                print(f"FEATURE SELECTION")
                print(f"{'='*80}")

                available_features = [f for f in CORE_FEATURES if f in df.columns]
                print(f"\n‚úì Available features: {len(available_features)}/{len(CORE_FEATURES)}")
                
                if len(available_features) < len(CORE_FEATURES):
                    missing = [f for f in CORE_FEATURES if f not in df.columns]
                    print(f"\n‚ö†Ô∏è  Missing features: {missing}")
                
                feature_cols = available_features

                print(f"\n{'='*80}")
                print(f"FILTERING TIME SERIES")
                print(f"{'='*80}")

                series_lengths = df.groupby(['country', 'industry']).size()
                valid_series = series_lengths[series_lengths >= args.min_years].index
                print(f"\n  Total series: {len(series_lengths)}")
                print(f"  Valid series (‚â•{args.min_years} years): {len(valid_series)}")

                df_filtered = df.set_index(['country', 'industry']).loc[valid_series].reset_index()
                print(f"‚úì After filtering: {df_filtered.shape}")

                print(f"\n{'='*80}")
                print(f"HANDLING MISSING VALUES")
                print(f"{'='*80}")

                df_sorted = df_filtered.sort_values(['country', 'industry', 'year']).copy()
                
                # Forward fill then backward fill within groups
                df_sorted[feature_cols] = df_sorted.groupby(['country', 'industry'])[feature_cols].ffill()
                df_sorted[feature_cols] = df_sorted.groupby(['country', 'industry'])[feature_cols].bfill()

                # Fill any remaining nulls with median
                for col in feature_cols:
                    if df_sorted[col].isnull().any():
                        median_val = df_sorted[col].median()
                        df_sorted[col].fillna(median_val if not pd.isna(median_val) else 0, inplace=True)

                print(f"‚úì All missing values imputed")

                print(f"\n{'='*80}")
                print(f"AUTOMATIC OUTLIER HANDLING")
                print(f"{'='*80}")

                log_transformed = []
                clipped = []

                for col in feature_cols:
                    min_val = df_sorted[col].min()
                    max_val = df_sorted[col].max()
                    range_val = max_val - min_val
                    
                    # Log transform for very large ranges
                    if range_val > args.outlier_log_threshold and max_val > 100:
                        print(f"  {col}: LOG TRANSFORM (range={range_val:.2f})")
                        df_sorted[col] = np.log1p(np.abs(df_sorted[col]))
                        log_transformed.append(col)
                    
                    # Clip outliers for moderate ranges
                    elif range_val > args.outlier_clip_threshold:
                        p999 = df_sorted[col].quantile(0.999)
                        p001 = df_sorted[col].quantile(0.001)
                        if max_val > p999 * 2:
                            print(f"  {col}: CLIP (range={range_val:.2f})")
                            df_sorted[col] = df_sorted[col].clip(lower=p001, upper=p999)
                            clipped.append(col)

                print(f"\n‚úì Log-transformed: {len(log_transformed)} features")
                print(f"‚úì Clipped outliers: {len(clipped)} features")

                print(f"\n{'='*80}")
                print(f"REMOVING ZERO-VARIANCE FEATURES")
                print(f"{'='*80}")

                feature_variance = df_sorted[feature_cols].var()
                low_var_features = feature_variance[feature_variance < 1e-6].index.tolist()

                if low_var_features:
                    print(f"\n‚ö†Ô∏è  Removing {len(low_var_features)} zero-variance features:")
                    for feat in low_var_features:
                        print(f"  - {feat}")
                    feature_cols = [f for f in feature_cols if f not in low_var_features]

                print(f"\n‚úì Final feature count: {len(feature_cols)}")

                print(f"\n{'='*80}")
                print(f"TRAIN/TEST SPLIT")
                print(f"{'='*80}")

                train_data = []
                test_data = []

                for (country, industry), group in df_sorted.groupby(['country', 'industry']):
                    group = group.sort_values('year')
                    n_years = len(group)
                    split_idx = int(n_years * (1 - args.test_split_pct))
                    
                    # Keep short series in training only
                    if split_idx < 10:
                        train_data.append(group)
                    elif n_years - split_idx < 3:
                        train_data.append(group)
                    else:
                        train_data.append(group.iloc[:split_idx])
                        test_data.append(group.iloc[split_idx:])

                train_df = pd.concat(train_data, ignore_index=True)
                test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame()

                print(f"\n‚úì Split complete:")
                print(f"  Training: {len(train_df)} records")
                print(f"  Test: {len(test_df)} records")

                if len(test_df) > 0:
                    test_lengths = test_df.groupby(['country', 'industry']).size()
                    print(f"  Test series: min={test_lengths.min()}, max={test_lengths.max()}, mean={test_lengths.mean():.1f} years")

                print(f"\n{'='*80}")
                print(f"STANDARDIZATION (StandardScaler)")
                print(f"{'='*80}")

                scaler = StandardScaler()

                print(f"\nBefore scaling (train):")
                print(f"  Mean: {train_df[feature_cols].mean().mean():.2f}")
                print(f"  Std: {train_df[feature_cols].std().mean():.2f}")

                scaler.fit(train_df[feature_cols])
                train_df[feature_cols] = scaler.transform(train_df[feature_cols])

                if len(test_df) > 0:
                    test_df[feature_cols] = scaler.transform(test_df[feature_cols])

                print(f"\nAfter scaling (train):")
                train_mean = train_df[feature_cols].mean().mean()
                train_std = train_df[feature_cols].std().mean()
                print(f"  Mean: {train_mean:.4f} (target: 0.0000)")
                print(f"  Std: {train_std:.4f} (target: 1.0000)")

                if abs(train_mean) < 0.01 and abs(train_std - 1) < 0.1:
                    print(f"\n‚úÖ‚úÖ PERFECT STANDARDIZATION!")
                else:
                    print(f"\n‚ö†Ô∏è Standardization quality check")

                if len(test_df) > 0:
                    print(f"\nAfter scaling (test):")
                    print(f"  Mean: {test_df[feature_cols].mean().mean():.4f}")
                    print(f"  Std: {test_df[feature_cols].std().mean():.4f}")

                print(f"\n{'='*80}")
                print(f"CREATING TENSORS")
                print(f"{'='*80}")

                def create_tensors(df, label='train'):
                    series_data = []
                    series_info = []
                    
                    for (country, industry), group in df.groupby(['country', 'industry']):
                        group = group.sort_values('year')
                        features = group[feature_cols].values
                        years = group['year'].values
                        
                        series_data.append(features)
                        series_info.append({
                            'country': country,
                            'industry': industry,
                            'n_years': len(years),
                            'year_start': int(years[0]),
                            'year_end': int(years[-1]),
                            'readiness_class': group['readiness_class'].iloc[0] if 'readiness_class' in group.columns else 'UNKNOWN'
                        })
                    
                    lengths = np.array([len(ts) for ts in series_data])
                    print(f"‚úì Created {len(series_data)} {label} series (length: {lengths.min()}-{lengths.max()})")
                    return series_data, series_info

                train_series, train_info = create_tensors(train_df, 'train')
                test_series, test_info = create_tensors(test_df, 'test') if len(test_df) > 0 else ([], [])

                print(f"\n{'='*80}")
                print(f"SAVING OUTPUT FILES")
                print(f"{'='*80}")

                # Create output directory
                output_dir = args.processed_data
                os.makedirs(output_dir, exist_ok=True)

                # Prepare data package
                data_package = {
                    'train_series': train_series,
                    'test_series': test_series,
                    'train_info': train_info,
                    'test_info': test_info,
                    'feature_names': feature_cols,
                    'n_features': len(feature_cols),
                    'scaler': scaler,
                    'log_transformed_features': log_transformed,
                    'clipped_features': clipped,
                    'metadata': {
                        'n_train_series': len(train_series),
                        'n_test_series': len(test_series),
                        'n_features': len(feature_cols),
                        'min_years': args.min_years,
                        'test_split_pct': args.test_split_pct,
                        'scaler_type': 'StandardScaler',
                        'preprocessing_date': datetime.now().isoformat(),
                        'log_transformed_count': len(log_transformed),
                        'clipped_count': len(clipped)
                    }
                }

                # Save processed data
                processed_data_path = os.path.join(output_dir, 'rupturenet_processed_data.pkl')
                with open(processed_data_path, 'wb') as f:
                    pickle.dump(data_package, f)
                print(f"‚úì Saved: {processed_data_path}")

                # Save scaler separately
                scaler_path = os.path.join(output_dir, 'rupturenet_scaler.pkl')
                with open(scaler_path, 'wb') as f:
                    pickle.dump(scaler, f)
                print(f"‚úì Saved: {scaler_path}")

                # Save metadata as JSON
                metadata_path = os.path.join(output_dir, 'rupturenet_processed_metadata.json')
                with open(metadata_path, 'w') as f:
                    json.dump(data_package['metadata'], f, indent=2)
                print(f"‚úì Saved: {metadata_path}")

                # Save feature list
                feature_list_path = os.path.join(output_dir, 'rupturenet_feature_list.json')
                with open(feature_list_path, 'w') as f:
                    json.dump({'features': feature_cols}, f, indent=2)
                print(f"‚úì Saved: {feature_list_path}")

                print(f"\n{'='*80}")
                print(f"PREPROCESSING SUMMARY")
                print(f"{'='*80}")

                print(f"\nüìä Data Statistics:")
                print(f"  Input records: {len(df)}")
                print(f"  Filtered records: {len(df_filtered)}")
                print(f"  Train records: {len(train_df)}")
                print(f"  Test records: {len(test_df)}")
                print(f"  Train series: {len(train_series)}")
                print(f"  Test series: {len(test_series)}")

                print(f"\nüîß Feature Engineering:")
                print(f"  Total features: {len(feature_cols)}")
                print(f"  Log-transformed: {len(log_transformed)}")
                print(f"  Clipped: {len(clipped)}")
                print(f"  Removed (zero-variance): {len(low_var_features) if low_var_features else 0}")

                print(f"\nüìê Scaling:")
                print(f"  Scaler: StandardScaler")
                print(f"  Train mean: {train_mean:.6f}")
                print(f"  Train std: {train_std:.6f}")

                print(f"\nüíæ Output Files:")
                print(f"  1. rupturenet_processed_data.pkl")
                print(f"  2. rupturenet_scaler.pkl")
                print(f"  3. rupturenet_processed_metadata.json")
                print(f"  4. rupturenet_feature_list.json")

                print(f"\n‚ö†Ô∏è  NEXT STEPS:")
                print(f"  This preprocessed data is ready for:")
                print(f"  1. BOCPD model training")
                print(f"  2. LSTM Autoencoder training")
                print(f"  3. Rupture detection pipeline")

                print(f"\n‚úÖ Preprocessing complete!")
                print(f"{'='*80}")
                print(f"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"{'='*80}")
                
            except Exception as e:
                print(f"\n‚ùå Error occurred: {e}")
                import traceback
                traceback.print_exc()
                raise
    args:
      - --input_csv
      - {inputPath: input_csv}
      - --min_years
      - {inputValue: min_years}
      - --test_split_pct
      - {inputValue: test_split_pct}
      - --outlier_log_threshold
      - {inputValue: outlier_log_threshold}
      - --outlier_clip_threshold
      - {inputValue: outlier_clip_threshold}
      - --processed_data
      - {outputPath: processed_data}