name: RuptureNet_CPD_Preprocessing
description: Preprocesses time series data for RuptureNet CPD model - handles feature engineering, outlier detection, train/test splitting, and standardization using StandardScaler.
inputs:
  - {name: input_csv, type: Dataset, description: "Path to input CSV file containing rupturenet features (1980-2024)"}
  - {name: min_years, type: Integer, default: "15", description: "Minimum years required for valid time series"}
  - {name: test_split_pct, type: Float, default: "0.15", description: "Percentage of data to use for test set"}
  - {name: outlier_log_threshold, type: Float, default: "1000.0", description: "Range threshold for log transformation"}
  - {name: outlier_clip_threshold, type: Float, default: "100.0", description: "Range threshold for outlier clipping"}
outputs:
  - {name: processed_data, type: Dataset, description: "Directory containing all processed data files"}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pandas || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pandas --user
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet numpy || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet numpy --user
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet scikit-learn || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import StandardScaler
        import json
        from datetime import datetime
        import pickle
        import warnings
        import os
        import argparse
        warnings.filterwarnings('ignore')

        CORE_FEATURES = [
            'wb_gdp_current_usd', 'trade_total_usd', 'trade_intensity',
            'trade_balance_pct_gdp', 'export_import_ratio', 'patent_intensity',
            'patents_per_billion_gdp', 'tech_diversity_index', 'pestle_economic',
            'pestle_social', 'pestle_technology', 'pestle_overall',
            'wb_gdp_growth_annual_pct', 'wb_fdi_net_inflows_pct_gdp',
            'innovation_capacity_index', 'wb_gdp_current_usd_pct_change_1y',
            'wb_gdp_current_usd_rolling_std_5y', 'trade_intensity_zscore_5y',
            'patent_intensity_pct_change_1y', 'trade_shock_signal',
            'innovation_surge_signal', 'fdi_shock_signal', 'composite_rupture_signal',
        ]

        if __name__ == "__main__":
            parser = argparse.ArgumentParser()
            parser.add_argument('--input_csv', type=str, required=True)
            parser.add_argument('--min_years', type=int, default=15)
            parser.add_argument('--test_split_pct', type=float, default=0.15)
            parser.add_argument('--outlier_log_threshold', type=float, default=1000.0)
            parser.add_argument('--outlier_clip_threshold', type=float, default=100.0)
            parser.add_argument('--processed_data', type=str, required=True)
            args = parser.parse_args()

            try:
                sep = "=" * 80
                print(sep)
                print("RUPTURENET-CPD: DATA PREPROCESSING FOR MODEL TRAINING")
                print(sep)
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                print("\\nStarted:", timestamp)

                if os.path.isdir(args.input_csv):
                    input_file = os.path.join(args.input_csv, 'rupturenet_features_final_1980_2024.csv')
                else:
                    input_file = args.input_csv

                print("\\n" + sep)
                print("LOADING DATASET")
                print(sep)

                df = pd.read_csv(input_file)
                print("\\nLoaded dataset with shape:", df.shape)
                print("Columns (first 10):", list(df.columns)[:10])

                print("\\n" + sep)
                print("FEATURE SELECTION")
                print(sep)

                available_features = [f for f in CORE_FEATURES if f in df.columns]
                print("\\nAvailable features:", len(available_features), "/", len(CORE_FEATURES))
                
                if len(available_features) < len(CORE_FEATURES):
                    missing = [f for f in CORE_FEATURES if f not in df.columns]
                    print("\\nWarning - Missing features:", missing)
                
                feature_cols = available_features

                print("\\n" + sep)
                print("FILTERING TIME SERIES")
                print(sep)

                series_lengths = df.groupby(['country', 'industry']).size()
                valid_series = series_lengths[series_lengths >= args.min_years].index
                print("\\nTotal series:", len(series_lengths))
                print("Valid series (>=", args.min_years, "years):", len(valid_series))

                df_filtered = df.set_index(['country', 'industry']).loc[valid_series].reset_index()
                print("After filtering shape:", df_filtered.shape)

                print("\\n" + sep)
                print("HANDLING MISSING VALUES")
                print(sep)

                df_sorted = df_filtered.sort_values(['country', 'industry', 'year']).copy()
                
                df_sorted[feature_cols] = df_sorted.groupby(['country', 'industry'])[feature_cols].ffill()
                df_sorted[feature_cols] = df_sorted.groupby(['country', 'industry'])[feature_cols].bfill()

                for col in feature_cols:
                    if df_sorted[col].isnull().any():
                        median_val = df_sorted[col].median()
                        df_sorted[col].fillna(median_val if not pd.isna(median_val) else 0, inplace=True)

                print("\\nAll missing values imputed")

                print("\\n" + sep)
                print("AUTOMATIC OUTLIER HANDLING")
                print(sep)

                log_transformed = []
                clipped = []

                for col in feature_cols:
                    min_val = df_sorted[col].min()
                    max_val = df_sorted[col].max()
                    range_val = max_val - min_val
                    
                    if range_val > args.outlier_log_threshold and max_val > 100:
                        print("  " + col + ": LOG TRANSFORM (range=" + str(round(range_val, 2)) + ")")
                        df_sorted[col] = np.log1p(np.abs(df_sorted[col]))
                        log_transformed.append(col)
                    
                    elif range_val > args.outlier_clip_threshold:
                        p999 = df_sorted[col].quantile(0.999)
                        p001 = df_sorted[col].quantile(0.001)
                        if max_val > p999 * 2:
                            print("  " + col + ": CLIP (range=" + str(round(range_val, 2)) + ")")
                            df_sorted[col] = df_sorted[col].clip(lower=p001, upper=p999)
                            clipped.append(col)

                print("\\nLog-transformed:", len(log_transformed), "features")
                print("Clipped outliers:", len(clipped), "features")

                print("\\n" + sep)
                print("REMOVING ZERO-VARIANCE FEATURES")
                print(sep)

                feature_variance = df_sorted[feature_cols].var()
                low_var_features = feature_variance[feature_variance < 1e-6].index.tolist()

                if low_var_features:
                    print("\\nRemoving", len(low_var_features), "zero-variance features:")
                    for feat in low_var_features:
                        print("  -", feat)
                    feature_cols = [f for f in feature_cols if f not in low_var_features]

                print("\\nFinal feature count:", len(feature_cols))

                print("\\n" + sep)
                print("TRAIN/TEST SPLIT")
                print(sep)

                train_data = []
                test_data = []

                for (country, industry), group in df_sorted.groupby(['country', 'industry']):
                    group = group.sort_values('year')
                    n_years = len(group)
                    split_idx = int(n_years * (1 - args.test_split_pct))
                    
                    if split_idx < 10:
                        train_data.append(group)
                    elif n_years - split_idx < 3:
                        train_data.append(group)
                    else:
                        train_data.append(group.iloc[:split_idx])
                        test_data.append(group.iloc[split_idx:])

                train_df = pd.concat(train_data, ignore_index=True)
                test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame()

                print("\\nSplit complete:")
                print("  Training:", len(train_df), "records")
                print("  Test:", len(test_df), "records")

                if len(test_df) > 0:
                    test_lengths = test_df.groupby(['country', 'industry']).size()
                    print("  Test series: min=", test_lengths.min(), ", max=", test_lengths.max(), 
                          ", mean=", round(test_lengths.mean(), 1), "years")

                print("\\n" + sep)
                print("STANDARDIZATION (StandardScaler)")
                print(sep)

                scaler = StandardScaler()

                print("\\nBefore scaling (train):")
                print("  Mean:", round(train_df[feature_cols].mean().mean(), 2))
                print("  Std:", round(train_df[feature_cols].std().mean(), 2))

                scaler.fit(train_df[feature_cols])
                train_df[feature_cols] = scaler.transform(train_df[feature_cols])

                if len(test_df) > 0:
                    test_df[feature_cols] = scaler.transform(test_df[feature_cols])

                print("\\nAfter scaling (train):")
                train_mean = train_df[feature_cols].mean().mean()
                train_std = train_df[feature_cols].std().mean()
                print("  Mean:", round(train_mean, 4), "(target: 0.0000)")
                print("  Std:", round(train_std, 4), "(target: 1.0000)")

                if abs(train_mean) < 0.01 and abs(train_std - 1) < 0.1:
                    print("\\nPERFECT STANDARDIZATION!")
                else:
                    print("\\nStandardization quality check")

                if len(test_df) > 0:
                    print("\\nAfter scaling (test):")
                    print("  Mean:", round(test_df[feature_cols].mean().mean(), 4))
                    print("  Std:", round(test_df[feature_cols].std().mean(), 4))

                print("\\n" + sep)
                print("CREATING TENSORS")
                print(sep)

                def create_tensors(df, label='train'):
                    series_data = []
                    series_info = []
                    
                    for (country, industry), group in df.groupby(['country', 'industry']):
                        group = group.sort_values('year')
                        features = group[feature_cols].values
                        years = group['year'].values
                        
                        series_data.append(features)
                        series_info.append({
                            'country': country,
                            'industry': industry,
                            'n_years': len(years),
                            'year_start': int(years[0]),
                            'year_end': int(years[-1]),
                            'readiness_class': group['readiness_class'].iloc[0] if 'readiness_class' in group.columns else 'UNKNOWN'
                        })
                    
                    lengths = np.array([len(ts) for ts in series_data])
                    print("Created", len(series_data), label, "series (length:", 
                          lengths.min(), "-", lengths.max(), ")")
                    return series_data, series_info

                train_series, train_info = create_tensors(train_df, 'train')
                test_series, test_info = create_tensors(test_df, 'test') if len(test_df) > 0 else ([], [])

                print("\\n" + sep)
                print("SAVING OUTPUT FILES")
                print(sep)

                output_dir = args.processed_data
                os.makedirs(output_dir, exist_ok=True)

                data_package = {
                    'train_series': train_series,
                    'test_series': test_series,
                    'train_info': train_info,
                    'test_info': test_info,
                    'feature_names': feature_cols,
                    'n_features': len(feature_cols),
                    'scaler': scaler,
                    'log_transformed_features': log_transformed,
                    'clipped_features': clipped,
                    'metadata': {
                        'n_train_series': len(train_series),
                        'n_test_series': len(test_series),
                        'n_features': len(feature_cols),
                        'min_years': args.min_years,
                        'test_split_pct': args.test_split_pct,
                        'scaler_type': 'StandardScaler',
                        'preprocessing_date': datetime.now().isoformat(),
                        'log_transformed_count': len(log_transformed),
                        'clipped_count': len(clipped)
                    }
                }

                processed_data_path = os.path.join(output_dir, 'rupturenet_processed_data.pkl')
                with open(processed_data_path, 'wb') as f:
                    pickle.dump(data_package, f)
                print("Saved:", processed_data_path)

                scaler_path = os.path.join(output_dir, 'rupturenet_scaler.pkl')
                with open(scaler_path, 'wb') as f:
                    pickle.dump(scaler, f)
                print("Saved:", scaler_path)

                metadata_path = os.path.join(output_dir, 'rupturenet_processed_metadata.json')
                with open(metadata_path, 'w') as f:
                    json.dump(data_package['metadata'], f, indent=2)
                print("Saved:", metadata_path)

                feature_list_path = os.path.join(output_dir, 'rupturenet_feature_list.json')
                with open(feature_list_path, 'w') as f:
                    json.dump({'features': feature_cols}, f, indent=2)
                print("Saved:", feature_list_path)

                print("\\n" + sep)
                print("PREPROCESSING SUMMARY")
                print(sep)

                print("\\nData Statistics:")
                print("  Input records:", len(df))
                print("  Filtered records:", len(df_filtered))
                print("  Train records:", len(train_df))
                print("  Test records:", len(test_df))
                print("  Train series:", len(train_series))
                print("  Test series:", len(test_series))

                print("\\nFeature Engineering:")
                print("  Total features:", len(feature_cols))
                print("  Log-transformed:", len(log_transformed))
                print("  Clipped:", len(clipped))
                removed_count = len(low_var_features) if low_var_features else 0
                print("  Removed (zero-variance):", removed_count)

                print("\\nScaling:")
                print("  Scaler: StandardScaler")
                print("  Train mean:", round(train_mean, 6))
                print("  Train std:", round(train_std, 6))

                print("\\nOutput Files:")
                print("  1. rupturenet_processed_data.pkl")
                print("  2. rupturenet_scaler.pkl")
                print("  3. rupturenet_processed_metadata.json")
                print("  4. rupturenet_feature_list.json")

                print("\\nNEXT STEPS:")
                print("  This preprocessed data is ready for:")
                print("  1. BOCPD model training")
                print("  2. LSTM Autoencoder training")
                print("  3. Rupture detection pipeline")

                print("\\nPreprocessing complete!")
                print(sep)
                timestamp_end = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                print("Completed:", timestamp_end)
                print(sep)
                
            except Exception as e:
                print("\\nError occurred:", str(e))
                import traceback
                traceback.print_exc()
                raise
    args:
      - --input_csv
      - {inputPath: input_csv}
      - --min_years
      - {inputValue: min_years}
      - --test_split_pct
      - {inputValue: test_split_pct}
      - --outlier_log_threshold
      - {inputValue: outlier_log_threshold}
      - --outlier_clip_threshold
      - {inputValue: outlier_clip_threshold}
      - --processed_data
      - {outputPath: processed_data}
