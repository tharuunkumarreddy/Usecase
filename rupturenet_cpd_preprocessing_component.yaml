name: RuptureNet_Data_Preprocessing
description: |
  RuptureNet-CPD Data Preprocessing Component - Prepares time series data for BOCPD and LSTM training.
  Features: automatic outlier handling, time series filtering, train/test split, StandardScaler normalization.
  CRITICAL FIX: Uses StandardScaler instead of RobustScaler for perfect normalization.
inputs:
  - {name: csv_file, type: Dataset, description: "Path to input CSV file with rupturenet features (e.g., rupturenet_features_final_1980_2024.csv)"}
  - {name: min_years, type: Integer, default: "15", description: "Minimum number of years required for a valid time series"}
  - {name: test_split_pct, type: Float, default: "0.15", description: "Percentage of data to use for test set (0.0-1.0)"}
  - {name: outlier_log_threshold, type: Float, default: "1000.0", description: "Range threshold for applying log transformation"}
  - {name: outlier_clip_threshold, type: Float, default: "100.0", description: "Range threshold for applying outlier clipping"}
outputs:
  - {name: processed_data, type: Dataset, description: "Directory containing processed data artifacts (pkl and json files)"}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pandas numpy scikit-learn || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet pandas numpy scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import StandardScaler
        import json
        from datetime import datetime
        import pickle
        import warnings
        import argparse
        import os
        warnings.filterwarnings('ignore')

        # Core features for RuptureNet-CPD
        CORE_FEATURES = [
            'wb_gdp_current_usd', 'trade_total_usd', 'trade_intensity',
            'trade_balance_pct_gdp', 'export_import_ratio', 'patent_intensity',
            'patents_per_billion_gdp', 'tech_diversity_index', 'pestle_economic',
            'pestle_social', 'pestle_technology', 'pestle_overall',
            'wb_gdp_growth_annual_pct', 'wb_fdi_net_inflows_pct_gdp',
            'innovation_capacity_index', 'wb_gdp_current_usd_pct_change_1y',
            'wb_gdp_current_usd_rolling_std_5y', 'trade_intensity_zscore_5y',
            'patent_intensity_pct_change_1y', 'trade_shock_signal',
            'innovation_surge_signal', 'fdi_shock_signal', 'composite_rupture_signal',
        ]

        def create_tensors(df, feature_cols, label='train'):
            #Create time series tensors from grouped data#
            series_data = []
            series_info = []
            
            for (country, industry), group in df.groupby(['country', 'industry']):
                group = group.sort_values('year')
                features = group[feature_cols].values
                years = group['year'].values
                
                series_data.append(features)
                series_info.append({
                    'country': country,
                    'industry': industry,
                    'n_years': len(years),
                    'year_start': int(years[0]),
                    'year_end': int(years[-1]),
                    'readiness_class': group['readiness_class'].iloc[0] if 'readiness_class' in group.columns else 'UNKNOWN'
                })
            
            lengths = np.array([len(ts) for ts in series_data])
            print(f"âœ“ Created {len(series_data)} {label} series (length: {lengths.min()}-{lengths.max()})")
            return series_data, series_info

        if __name__ == "__main__":
            parser = argparse.ArgumentParser()
            parser.add_argument('--csv_file', type=str, required=True)
            parser.add_argument('--min_years', type=int, default=15)
            parser.add_argument('--test_split_pct', type=float, default=0.15)
            parser.add_argument('--outlier_log_threshold', type=float, default=1000.0)
            parser.add_argument('--outlier_clip_threshold', type=float, default=100.0)
            parser.add_argument('--processed_data', type=str, required=True)
            args = parser.parse_args()

            try:
                print("="*80)
                print("RUPTURENET-CPD: DATA PREPROCESSING FOR MODEL TRAINING")
                print("="*80)
                print(f"\\nStarted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

                # Handle input path
                if os.path.isdir(args.csv_file):
                    csv_path = os.path.join(args.csv_file, 'rupturenet_features_final_1980_2024.csv')
                else:
                    csv_path = args.csv_file

                print(f"\\n{'='*80}")
                print(f"LOADING DATASET")
                print(f"{'='*80}")

                df = pd.read_csv(csv_path)
                print(f"\\nâœ“ Loaded: {df.shape}")
                print(f"  Columns: {list(df.columns)[:10]}...")

                print(f"\\n{'='*80}")
                print(f"FEATURE SELECTION")
                print(f"{'='*80}")

                available_features = [f for f in CORE_FEATURES if f in df.columns]
                missing_features = [f for f in CORE_FEATURES if f not in df.columns]
                
                print(f"\\nâœ“ Available features: {len(available_features)}/{len(CORE_FEATURES)}")
                if missing_features:
                    print(f"âš ï¸  Missing features: {missing_features}")
                
                feature_cols = available_features

                print(f"\\n{'='*80}")
                print(f"FILTERING TIME SERIES")
                print(f"{'='*80}")

                series_lengths = df.groupby(['country', 'industry']).size()
                valid_series = series_lengths[series_lengths >= args.min_years].index
                print(f"\\n  Total series: {len(series_lengths)}")
                print(f"  Valid series (â‰¥{args.min_years} years): {len(valid_series)}")

                df_filtered = df.set_index(['country', 'industry']).loc[valid_series].reset_index()
                print(f"âœ“ After filtering: {df_filtered.shape}")

                print(f"\\n{'='*80}")
                print(f"HANDLING MISSING VALUES")
                print(f"{'='*80}")

                df_sorted = df_filtered.sort_values(['country', 'industry', 'year']).copy()
                
                # Forward fill then backward fill within each group
                df_sorted[feature_cols] = df_sorted.groupby(['country', 'industry'])[feature_cols].ffill()
                df_sorted[feature_cols] = df_sorted.groupby(['country', 'industry'])[feature_cols].bfill()

                # Fill any remaining NaNs with median
                for col in feature_cols:
                    if df_sorted[col].isnull().any():
                        median_val = df_sorted[col].median()
                        df_sorted[col].fillna(median_val if not pd.isna(median_val) else 0, inplace=True)

                print(f"âœ“ All missing values imputed")

                print(f"\\n{'='*80}")
                print(f"AUTOMATIC OUTLIER HANDLING")
                print(f"{'='*80}")

                log_transformed = []
                clipped = []

                for col in feature_cols:
                    min_val = df_sorted[col].min()
                    max_val = df_sorted[col].max()
                    range_val = max_val - min_val
                    
                    # Apply log transform for very large ranges
                    if range_val > args.outlier_log_threshold and max_val > 100:
                        print(f"  {col}: LOG TRANSFORM (range={range_val:.2f})")
                        df_sorted[col] = np.log1p(np.abs(df_sorted[col]))
                        log_transformed.append(col)
                    # Clip outliers for moderately large ranges
                    elif range_val > args.outlier_clip_threshold:
                        p999 = df_sorted[col].quantile(0.999)
                        p001 = df_sorted[col].quantile(0.001)
                        if max_val > p999 * 2:
                            print(f"  {col}: CLIP (range={range_val:.2f})")
                            df_sorted[col] = df_sorted[col].clip(lower=p001, upper=p999)
                            clipped.append(col)

                print(f"\\nâœ“ Log-transformed: {len(log_transformed)} features")
                print(f"âœ“ Clipped outliers: {len(clipped)} features")

                print(f"\\n{'='*80}")
                print(f"REMOVING ZERO-VARIANCE FEATURES")
                print(f"{'='*80}")

                feature_variance = df_sorted[feature_cols].var()
                low_var_features = feature_variance[feature_variance < 1e-6].index.tolist()

                if low_var_features:
                    print(f"\\nâš ï¸  Removing {len(low_var_features)} zero-variance features:")
                    for feat in low_var_features:
                        print(f"  - {feat}")
                    feature_cols = [f for f in feature_cols if f not in low_var_features]

                print(f"\\nâœ“ Final feature count: {len(feature_cols)}")

                print(f"\\n{'='*80}")
                print(f"TRAIN/TEST SPLIT")
                print(f"{'='*80}")

                train_data = []
                test_data = []

                for (country, industry), group in df_sorted.groupby(['country', 'industry']):
                    group = group.sort_values('year')
                    n_years = len(group)
                    split_idx = int(n_years * (1 - args.test_split_pct))
                    
                    # Don't split very short series
                    if split_idx < 10:
                        train_data.append(group)
                    # Don't create tiny test sets
                    elif n_years - split_idx < 3:
                        train_data.append(group)
                    else:
                        train_data.append(group.iloc[:split_idx])
                        test_data.append(group.iloc[split_idx:])

                train_df = pd.concat(train_data, ignore_index=True)
                test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame()

                print(f"\\nâœ“ Split complete:")
                print(f"  Training: {len(train_df)} records")
                print(f"  Test: {len(test_df)} records")

                if len(test_df) > 0:
                    test_lengths = test_df.groupby(['country', 'industry']).size()
                    print(f"  Test series: min={test_lengths.min()}, max={test_lengths.max()}, mean={test_lengths.mean():.1f} years")

                print(f"\\n{'='*80}")
                print(f"STANDARDIZATION (CRITICAL FIX!)")
                print(f"{'='*80}")

                # CRITICAL: StandardScaler for perfect mean=0, std=1 normalization
                scaler = StandardScaler()

                print(f"\\nBefore scaling (train):")
                print(f"  Mean: {train_df[feature_cols].mean().mean():.2f}")
                print(f"  Std: {train_df[feature_cols].std().mean():.2f}")

                scaler.fit(train_df[feature_cols])
                train_df[feature_cols] = scaler.transform(train_df[feature_cols])

                if len(test_df) > 0:
                    test_df[feature_cols] = scaler.transform(test_df[feature_cols])

                print(f"\\nAfter scaling (train):")
                train_mean = train_df[feature_cols].mean().mean()
                train_std = train_df[feature_cols].std().mean()
                print(f"  Mean: {train_mean:.4f} (target: 0.0000)")
                print(f"  Std: {train_std:.4f} (target: 1.0000)")

                if abs(train_mean) < 0.01 and abs(train_std - 1) < 0.1:
                    print(f"\\nâœ…âœ… PERFECT STANDARDIZATION!")
                else:
                    print(f"\\nâš ï¸ Standardization quality check")

                if len(test_df) > 0:
                    print(f"\\nAfter scaling (test):")
                    print(f"  Mean: {test_df[feature_cols].mean().mean():.4f}")
                    print(f"  Std: {test_df[feature_cols].std().mean():.4f}")

                print(f"\\n{'='*80}")
                print(f"CREATING TENSORS")
                print(f"{'='*80}")

                train_series, train_info = create_tensors(train_df, feature_cols, 'train')
                test_series, test_info = create_tensors(test_df, feature_cols, 'test') if len(test_df) > 0 else ([], [])

                print(f"\\n{'='*80}")
                print(f"SAVING OUTPUT ARTIFACTS")
                print(f"{'='*80}")

                # Create output directory
                output_dir = args.processed_data
                os.makedirs(output_dir, exist_ok=True)

                # Package all data
                data_package = {
                    'train_series': train_series,
                    'test_series': test_series,
                    'train_info': train_info,
                    'test_info': test_info,
                    'feature_names': feature_cols,
                    'n_features': len(feature_cols),
                    'scaler': scaler,
                    'log_transformed_features': log_transformed,
                    'clipped_features': clipped,
                    'metadata': {
                        'n_train_series': len(train_series),
                        'n_test_series': len(test_series),
                        'n_features': len(feature_cols),
                        'min_years': args.min_years,
                        'test_split_pct': args.test_split_pct,
                        'scaler_type': 'StandardScaler',
                        'preprocessing_date': datetime.now().isoformat()
                    }
                }

                # Save main data package
                processed_data_path = os.path.join(output_dir, 'rupturenet_processed_data.pkl')
                with open(processed_data_path, 'wb') as f:
                    pickle.dump(data_package, f)
                print(f"âœ“ Saved: {processed_data_path}")

                # Save scaler separately
                scaler_path = os.path.join(output_dir, 'rupturenet_scaler.pkl')
                with open(scaler_path, 'wb') as f:
                    pickle.dump(scaler, f)
                print(f"âœ“ Saved: {scaler_path}")

                # Save metadata as JSON
                metadata_path = os.path.join(output_dir, 'rupturenet_processed_metadata.json')
                with open(metadata_path, 'w') as f:
                    json.dump(data_package['metadata'], f, indent=2)
                print(f"âœ“ Saved: {metadata_path}")

                # Save feature list
                feature_list_path = os.path.join(output_dir, 'rupturenet_feature_list.json')
                with open(feature_list_path, 'w') as f:
                    json.dump({'features': feature_cols}, f, indent=2)
                print(f"âœ“ Saved: {feature_list_path}")

                print(f"\\n{'='*80}")
                print(f"PREPROCESSING COMPLETE")
                print(f"{'='*80}")

                print(f"\\nğŸ“Š Summary:")
                print(f"  Features: {len(feature_cols)}")
                print(f"  Log-transformed: {len(log_transformed)}")
                print(f"  Clipped: {len(clipped)}")
                print(f"  Train series: {len(train_series)}")
                print(f"  Test series: {len(test_series)}")
                print(f"  Scaler: StandardScaler (for perfect normalization)")
                print(f"  Output directory: {output_dir}")

                print(f"\\nâœ… All artifacts saved successfully!")
                print(f"\\n{'='*80}")
                print(f"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"{'='*80}")
                
            except Exception as e:
                print(f"\\nâŒ Error occurred: {e}")
                import traceback
                traceback.print_exc()
                raise
    args:
      - --csv_file
      - {inputPath: csv_file}
      - --min_years
      - {inputValue: min_years}
      - --test_split_pct
      - {inputValue: test_split_pct}
      - --outlier_log_threshold
      - {inputValue: outlier_log_threshold}
      - --outlier_clip_threshold
      - {inputValue: outlier_clip_threshold}
      - --processed_data
      - {outputPath: processed_data}

