name: RuptureNet_LSTM_to_ONNX
description: Converts RuptureNet LSTM Autoencoder PyTorch model to ONNX format. Based on MPQE and CNN conversion patterns. Handles flexible checkpoint loading and comprehensive validation.

inputs:
  - name: lstm_model
    type: Dataset
    description: "LSTM checkpoint from training pipeline (Dataset with .pth file)"
  - name: processed_data  
    type: Dataset
    description: "Preprocessed data directory with rupturenet_processed_data.pkl"
  - name: output_filename
    type: String
    default: "model.onnx"
    description: "Output ONNX filename (use model.onnx for Triton compatibility)"

outputs:
  - name: onnx_model
    type: Model
    description: "ONNX model ready for KServe/Triton deployment"

implementation:
  container:
    image: nikhilv215/nesy-factory:testv2
    command:
      - sh
      - -c
      - |
        pip3 install --no-cache-dir onnx onnxruntime
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import glob
        import torch
        import torch.nn as nn
        import numpy as np
        import pickle
        import json
        from datetime import datetime

        parser = argparse.ArgumentParser()
        parser.add_argument('--lstm_model', required=True)
        parser.add_argument('--processed_data', required=True)
        parser.add_argument('--output_filename', default='model.onnx')
        parser.add_argument('--onnx_model', required=True)
        args = parser.parse_args()

        print("="*80)
        print("RUPTURENET LSTM TO ONNX CONVERSION")
        print("="*80)
        now = datetime.now()
        print("Started:", now.strftime("%Y-%m-%d %H:%M:%S"))
        print("LSTM model path:", args.lstm_model)
        print("Processed data path:", args.processed_data)
        print("Output filename:", args.output_filename)

        class LSTMAutoencoder(nn.Module):
            def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, dropout):
                super().__init__()
                self.input_dim = input_dim
                self.hidden_dim = hidden_dim
                self.latent_dim = latent_dim
                self.num_layers = num_layers
                
                self.encoder_lstm = nn.LSTM(
                    input_size=input_dim,
                    hidden_size=hidden_dim,
                    num_layers=num_layers,
                    dropout=dropout if num_layers > 1 else 0,
                    batch_first=True
                )
                self.encoder_to_latent = nn.Linear(hidden_dim, latent_dim)
                
                self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)
                self.decoder_lstm = nn.LSTM(
                    input_size=hidden_dim,
                    hidden_size=hidden_dim,
                    num_layers=num_layers,
                    dropout=dropout if num_layers > 1 else 0,
                    batch_first=True
                )
                self.decoder_output = nn.Linear(hidden_dim, input_dim)
            
            def forward(self, x, seq_len):
                batch_size, max_seq_len = x.size(0), x.size(1)
                
                packed_input = nn.utils.rnn.pack_padded_sequence(
                    x, seq_len.cpu(), batch_first=True, enforce_sorted=False
                )
                _, (hidden, _) = self.encoder_lstm(packed_input)
                latent = self.encoder_to_latent(hidden[-1])
                
                decoder_input = self.latent_to_hidden(latent)
                decoder_input = decoder_input.unsqueeze(1).repeat(1, max_seq_len, 1)
                
                packed_decoder = nn.utils.rnn.pack_padded_sequence(
                    decoder_input, seq_len.cpu(), batch_first=True, enforce_sorted=False
                )
                packed_output, _ = self.decoder_lstm(packed_decoder)
                decoder_output, _ = nn.utils.rnn.pad_packed_sequence(
                    packed_output, batch_first=True, total_length=max_seq_len
                )
                reconstructed = self.decoder_output(decoder_output)
                
                return reconstructed, latent

        print("")
        print("="*80)
        print("LOADING CHECKPOINT")
        print("="*80)
        
        if os.path.isdir(args.lstm_model):
            print("Input is directory:", args.lstm_model)
            
            preferred = os.path.join(args.lstm_model, "lstm_autoencoder_best.pth")
            if os.path.isfile(preferred):
                ckpt_path = preferred
                print("Using preferred file: lstm_autoencoder_best.pth")
            else:
                candidates = glob.glob(os.path.join(args.lstm_model, "*.pth")) + glob.glob(os.path.join(args.lstm_model, "*.pt"))
                if not candidates:
                    raise FileNotFoundError("No .pth or .pt files found in " + args.lstm_model)
                ckpt_path = candidates[0]
                print("Using auto-detected file:", os.path.basename(ckpt_path))
        else:
            ckpt_path = args.lstm_model
            print("Input is file:", ckpt_path)
        
        if not os.path.isfile(ckpt_path):
            raise FileNotFoundError("Checkpoint not found: " + ckpt_path)
        
        print("Loading checkpoint:", ckpt_path)
        size_mb = os.path.getsize(ckpt_path) / 1024 / 1024
        print("File size: {:.2f} MB".format(size_mb))
        
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        
        print("")
        print("="*80)
        print("EXTRACTING CONFIGURATION")
        print("="*80)
        
        if isinstance(checkpoint, dict):
            if 'config' in checkpoint:
                config = checkpoint['config']
                print("Found config in checkpoint")
            else:
                print("No config key, using defaults")
                config = {}
            
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                print("Found model_state_dict in checkpoint")
            else:
                state_dict = checkpoint
                print("Using entire checkpoint as state_dict")
        else:
            config = {}
            state_dict = checkpoint
            print("Checkpoint is raw state_dict")
        
        input_dim = config.get('input_dim', 10)
        hidden_dim = config.get('hidden_dim', 64)
        latent_dim = config.get('latent_dim', 32)
        num_layers = config.get('num_layers', 2)
        dropout = config.get('dropout', 0.2)
        
        print("")
        print("Model configuration:")
        print("  input_dim:", input_dim)
        print("  hidden_dim:", hidden_dim)
        print("  latent_dim:", latent_dim)
        print("  num_layers:", num_layers)
        print("  dropout:", dropout)
        
        print("")
        print("="*80)
        print("CREATING MODEL")
        print("="*80)
        
        model = LSTMAutoencoder(input_dim, hidden_dim, latent_dim, num_layers, dropout)
        
        print("Loading state_dict with strict=False...")
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        
        if missing_keys:
            print("Missing keys:", missing_keys)
        if unexpected_keys:
            print("Unexpected keys:", unexpected_keys)
        
        if not missing_keys and not unexpected_keys:
            print("All keys matched perfectly")
        
        model.eval()
        print("Model ready for export")
        
        print("")
        print("="*80)
        print("LOADING SAMPLE DATA")
        print("="*80)
        
        data_path = os.path.join(args.processed_data, 'rupturenet_processed_data.pkl')
        with open(data_path, 'rb') as f:
            data_package = pickle.load(f)
        
        test_series = data_package['test_series']
        feature_names = data_package['feature_names']
        
        seq_lengths = [len(s) for s in test_series]
        avg_seq_len = int(np.mean(seq_lengths))
        max_seq_len = max(seq_lengths)
        
        print("Loaded {} test series".format(len(test_series)))
        print("Sequence lengths: avg={}, max={}".format(avg_seq_len, max_seq_len))
        print("Features:", len(feature_names))
        
        class ONNXWrapper(nn.Module):
            def __init__(self, model):
                super().__init__()
                self.model = model
            
            def forward(self, x, seq_len):
                reconstructed, latent = self.model(x, seq_len)
                reconstruction_error = torch.mean((x - reconstructed) ** 2, dim=2)
                return reconstruction_error, latent
        
        wrapper = ONNXWrapper(model)
        wrapper.eval()
        print("")
        print("Created ONNX wrapper")
        
        batch_size = 2
        dummy_x = torch.randn(batch_size, max_seq_len, input_dim)
        dummy_seq_len = torch.tensor([max_seq_len, max_seq_len-5], dtype=torch.long)
        
        print("")
        print("Dummy input shapes:")
        print("  time_series: {} (batch, seq_len, features)".format(dummy_x.shape))
        print("  sequence_lengths:", dummy_seq_len.shape)
        
        print("")
        print("="*80)
        print("EXPORTING TO ONNX")
        print("="*80)
        
        os.makedirs(args.onnx_model, exist_ok=True)
        output_path = os.path.join(args.onnx_model, args.output_filename)
        
        print("Exporting to:", output_path)
        
        torch.onnx.export(
            wrapper,
            (dummy_x, dummy_seq_len),
            output_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input', 'sequence_lengths'],
            output_names=['reconstruction_error', 'latent'],
            dynamic_axes={
                'input': {0: 'batch_size', 1: 'seq_len'},
                'sequence_lengths': {0: 'batch_size'},
                'reconstruction_error': {0: 'batch_size', 1: 'seq_len'},
                'latent': {0: 'batch_size'}
            }
        )
        
        file_size = os.path.getsize(output_path) / 1024 / 1024
        print("ONNX export complete")
        print("  Size: {:.2f} MB".format(file_size))
        
        print("")
        print("="*80)
        print("VALIDATING ONNX MODEL")
        print("="*80)
        
        import onnx
        import onnxruntime as ort
        
        onnx_model = onnx.load(output_path)
        onnx.checker.check_model(onnx_model)
        print("ONNX model structure is valid")
        
        print("")
        print("Testing ONNX Runtime inference...")
        session = ort.InferenceSession(output_path)
        
        test_cases = [
            (1, 15),
            (2, 20),
            (1, 30),
            (4, 25),
        ]
        
        all_passed = True
        for batch, seq_len in test_cases:
            try:
                test_x = np.random.randn(batch, seq_len, input_dim).astype(np.float32)
                test_seq = np.array([seq_len] * batch, dtype=np.int64)
                
                outputs = session.run(
                    ['reconstruction_error', 'latent'],
                    {
                        'input': test_x,
                        'sequence_lengths': test_seq
                    }
                )
                
                error_shape = outputs[0].shape
                latent_shape = outputs[1].shape
                
                msg = "  Test [{}x{}x{}]: error={}, latent={}"
                print(msg.format(batch, seq_len, input_dim, error_shape, latent_shape))
            except Exception as e:
                msg = "  Test [{}x{}x{}] FAILED: {}"
                print(msg.format(batch, seq_len, input_dim, str(e)))
                all_passed = False
        
        if not all_passed:
            raise RuntimeError("Some ONNX inference tests failed")
        
        print("")
        print("All inference tests passed")
        
        metadata = {
            'model_type': 'LSTM Autoencoder',
            'framework': 'PyTorch',
            'architecture': {
                'input_dim': input_dim,
                'hidden_dim': hidden_dim,
                'latent_dim': latent_dim,
                'num_layers': num_layers,
                'dropout': dropout
            },
            'onnx': {
                'opset_version': 11,
                'file_size_mb': file_size,
                'filename': args.output_filename
            },
            'inputs': {
                'input': '[batch_size, seq_len, {0}]'.format(input_dim),
                'sequence_lengths': '[batch_size]'
            },
            'outputs': {
                'reconstruction_error': '[batch_size, seq_len]',
                'latent': '[batch_size, {0}]'.format(latent_dim)
            },
            'features': feature_names,
            'conversion_date': datetime.now().isoformat()
        }
        
        metadata_path = os.path.join(args.onnx_model, 'metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print("")
        print("Saved metadata:", metadata_path)
        
        print("")
        print("="*80)
        print("CONVERSION COMPLETE")
        print("="*80)
        print("")
        print("Output files:")
        print(" ", output_path)
        print(" ", metadata_path)
        print("")
        print("Ready for KServe/Triton deployment!")
        end = datetime.now()
        print("Completed:", end.strftime("%Y-%m-%d %H:%M:%S"))
        print("="*80)

    args:
      - --lstm_model
      - {inputPath: lstm_model}
      - --processed_data
      - {inputPath: processed_data}
      - --output_filename
      - {inputValue: output_filename}
      - --onnx_model
      - {outputPath: onnx_model}
